<h2>Toxicity</h2>
<p>モデルtoxicityを使った有害なテキストの判別</p>

<h3>入力</h3>
<p>テキスト</p>

<h3>出力</h3>
<p>下記の点での評価</p>
<ul>
  <li>toxicity: 有害</li>
  <li>severe_toxicity: 酷く有害</li>
  <li>identity_attack: アイデンティティへの攻撃</li>
  <li>insult: 侮辱</li>
  <li>obscene: 卑猥</li>
  <li>sexual_explicit: 性的に露骨</li>
  <li>threat: 脅迫</li>
</ul>

<h3>実行手順</h3>
<ol>
  <li>npm install @tensorflow/tfjs @tensorflow-models/toxicity</li>
  <li>評価のための閾値(0~1)を指定してモデルの読み込み</li>
  <li>モデルにテキストを渡す</li>
</ol>


<h3>DEMO</h3>
<p>閾値は0.8を指定</p>
<p>例：</p>
<ul>
  <li>what the fuck</li>
  <li>Holy shit!</li>
  <li>I will flay you alive, you fking faggot.</li>
</ul>

<p>入力：</p>
<input type="text" [(ngModel)]="toxicity_text">

<p>出力：</p>
<p>{{toxicity_text | toxicity | async | json}}</p>
